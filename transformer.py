# -*- coding: utf-8 -*-
"""Ariola_Leka_DLL4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MO2knjCig3rVypxfpC1tl29cl753mm_Z
"""

import os
import math
import time
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class Vocabulary:

    def __init__(self, pad_token="<pad>", unk_token='<unk>', eos_token='<eos>', sos_token='<sos>'):
        self.id_to_string = {}
        self.string_to_id = {}
        
        # add the default pad token
        self.id_to_string[0] = pad_token
        self.string_to_id[pad_token] = 0
        
        # add the default unknown token
        self.id_to_string[1] = unk_token
        self.string_to_id[unk_token] = 1
        
        # add the default unknown token
        self.id_to_string[2] = eos_token
        self.string_to_id[eos_token] = 2   

        # add the default unknown token
        self.id_to_string[3] = sos_token
        self.string_to_id[sos_token] = 3

        # shortcut access
        self.pad_id = 0
        self.unk_id = 1
        self.eos_id = 2
        self.sos_id = 3

    def __len__(self):
        return len(self.id_to_string)

    def add_new_word(self, string):
        self.string_to_id[string] = len(self.string_to_id)
        self.id_to_string[len(self.id_to_string)] = string

    # Given a string, return ID
    # if extend_vocab is True, add the new word
    def get_idx(self, string, extend_vocab=False):
        if string in self.string_to_id:
            return self.string_to_id[string]
        elif extend_vocab:  # add the new word
            self.add_new_word(string)
            return self.string_to_id[string]
        else:
            return self.unk_id


# Read the raw txt file and generate a 1D pytorch tensor
# containing the whole text mapped to sequence of token ID,
# and a vocab file
class ParallelTextDataset(Dataset):

    def __init__(self, src_file_path, trg_file_path, src_vocab=None,
                 trg_vocab=None, extend_vocab=False, device='cuda'):
        (self.data, self.src_vocab, self.trg_vocab,
         self.src_max_seq_length, self.tgt_max_seq_length) = self.parallel_text_to_data(
            src_file_path, trg_file_path, src_vocab, trg_vocab, extend_vocab, device)

    def __getitem__(self, idx):
        return self.data[idx]

    def __len__(self):
        return len(self.data)

    def parallel_text_to_data(self, src_file, tgt_file, src_vocab=None, tgt_vocab=None,
                          extend_vocab=False, device='cuda'):
        # Convert paired src/tgt texts into torch.tensor data.
        # All sequences are padded to the length of the longest sequence
        # of the respective file.
        print(f"src{src_file}")

        assert os.path.exists(src_file)
        assert os.path.exists(tgt_file)

        if src_vocab is None:
            src_vocab = Vocabulary()

        if tgt_vocab is None:
            tgt_vocab = Vocabulary()
        
        data_list = []
        # Check the max length, if needed construct vocab file.
        src_max = 0
        with open(src_file, 'r') as text:
            for line in text:
                tokens = list(line)[:-1]
                length = len(tokens)
                if src_max < length:
                    src_max = length

        tgt_max = 0
        with open(tgt_file, 'r') as text:
            for line in text:
                tokens = list(line)[:-1]
                length = len(tokens)
                if tgt_max < length:
                    tgt_max = length
        tgt_max += 2  # add for begin/end tokens
                    
        src_pad_idx = src_vocab.pad_id
        tgt_pad_idx = tgt_vocab.pad_id

        tgt_eos_idx = tgt_vocab.eos_id
        tgt_sos_idx = tgt_vocab.sos_id

        # Construct data
        src_list = []
        print(f"Loading source file from: {src_file}")
        with open(src_file, 'r') as text:
            for line in tqdm(text):
                seq = []
                tokens = list(line)[:-1]
                for token in tokens:
                    seq.append(src_vocab.get_idx(token, extend_vocab=extend_vocab))
                var_len = len(seq)
                var_seq = torch.tensor(seq, device=device, dtype=torch.int64)
                # padding
                new_seq = var_seq.data.new(src_max).fill_(src_pad_idx)
                new_seq[:var_len] = var_seq
                src_list.append(new_seq)

        tgt_list = []
        print(f"Loading target file from: {tgt_file}")
        with open(tgt_file, 'r') as text:
            for line in tqdm(text):
                seq = []
                tokens = list(line)[:-1]
                # append a start token
                seq.append(tgt_sos_idx)
                for token in tokens:
                    seq.append(tgt_vocab.get_idx(token, extend_vocab=extend_vocab))
                # append an end token
                seq.append(tgt_eos_idx)

                var_len = len(seq)
                var_seq = torch.tensor(seq, device=device, dtype=torch.int64)

                # padding
                new_seq = var_seq.data.new(tgt_max).fill_(tgt_pad_idx)
                new_seq[:var_len] = var_seq
                tgt_list.append(new_seq)

        # src_file and tgt_file are assumed to be aligned.
        assert len(src_list) == len(tgt_list)
        for i in range(len(src_list)):
            data_list.append((src_list[i], tgt_list[i]))

        print("Done.")
            
        return data_list, src_vocab, tgt_vocab, src_max, tgt_max

#!unzip "/home/Datasets for Assignment 4-20230102.zip" 
#!unzip "/content/numbers__place_value.zip"

# `DATASET_DIR` should be modified to the directory where you downloaded the dataset.

DATASET_DIR = "/content"

TRAIN_FILE_NAME = "train"
VALID_FILE_NAME = "interpolate"

INPUTS_FILE_ENDING = ".x"
TARGETS_FILE_ENDING = ".y"

TASK = "numbers__place_value"
# TASK = "comparison__sort"
# TASK = "algebra__linear_1d"

# Adapt the paths!

src_file_path = f"{DATASET_DIR}/{TASK}/{TRAIN_FILE_NAME}{INPUTS_FILE_ENDING}"
trg_file_path = f"{DATASET_DIR}/{TASK}/{TRAIN_FILE_NAME}{TARGETS_FILE_ENDING}"

train_set = ParallelTextDataset(src_file_path, trg_file_path, extend_vocab=True)

# get the vocab
src_vocab = train_set.src_vocab
trg_vocab = train_set.trg_vocab


src_file_path = f"{DATASET_DIR}/{TASK}/{VALID_FILE_NAME}{INPUTS_FILE_ENDING}"
trg_file_path = f"{DATASET_DIR}/{TASK}/{VALID_FILE_NAME}{TARGETS_FILE_ENDING}"

valid_set = ParallelTextDataset(
    src_file_path, trg_file_path, src_vocab=src_vocab, trg_vocab=trg_vocab,
    extend_vocab=False)

print(len(src_vocab))
print(len(trg_vocab))

src_vocab.id_to_string

trg_vocab.id_to_string

########
# Taken from:
# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
# or also here:
# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
class PositionalEncoding(nn.Module):

    def __init__(self, d_model, dropout=0.0, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        self.max_len = max_len

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float()
                             * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
        self.register_buffer('pe', pe)  # Will not be trained.

    def forward(self, x):
        """Inputs of forward function
        Args:
            x: the sequence fed to the positional encoder model (required).
        Shape:
            x: [sequence length, batch size, embed dim]
            output: [sequence length, batch size, embed dim]
        """
        assert x.size(0) < self.max_len, (
            f"Too long sequence length: increase `max_len` of pos encoding")
        # shape of x (len, B, dim)
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class TransformerModel(nn.Module):
    def __init__(self, source_vocabulary_size, target_vocabulary_size,
                 d_model=256, pad_id=0, encoder_layers=3, decoder_layers=2,
                 dim_feedforward=1024, num_heads=8):
        # all arguments are (int)
        super().__init__()
        self.pad_id = pad_id

        self.embedding_src = nn.Embedding(
            source_vocabulary_size, d_model, padding_idx=pad_id)
        self.embedding_tgt = nn.Embedding(
            target_vocabulary_size, d_model, padding_idx=pad_id)

        self.pos_encoder = PositionalEncoding(d_model)
        self.transformer = nn.Transformer(
            d_model, num_heads, encoder_layers, decoder_layers, dim_feedforward)
        self.encoder = self.transformer.encoder
        self.decoder = self.transformer.decoder
        self.linear = nn.Linear(d_model, target_vocabulary_size)

    def create_src_padding_mask(src):
        # input src of shape ()
        src_padding_mask = src.transpose(0, 1) == 0
        return src_padding_mask

    def create_tgt_padding_mask(tgt):
        # input tgt of shape ()
        tgt_padding_mask = tgt.transpose(0, 1) == 0
        return tgt_padding_mask

    # Implement me!
    # def greedy_seach():

    def greedy_search(self, src, max_len, id_sos, id_eos): #max_len presumabily should be 3, three tokens, sos, place of number, eos
        src=src.view(50,1)

        #forward the encoder once
        src_key_padding_mask = TransformerModel.create_src_padding_mask(src).to(DEVICE)
        memory_key_padding_mask = src_key_padding_mask
        emb = self.embedding_src(src)
        pos_enc = self.pos_encoder(emb)
        memory = self.encoder(pos_enc, src_key_padding_mask=src_key_padding_mask)
        #print(f"memory {memory}")

        decoder_input = torch.ones((1,1)).fill_(id_sos).type(torch.long).to(DEVICE)
        tgt = decoder_input

        # print(len(memory))

        while True:

          tgt_key_padding_mask = TransformerModel.create_tgt_padding_mask(tgt).to(DEVICE)
          tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.shape[0]).to(DEVICE)
          tgt = self.embedding_tgt(tgt)
          tgt = self.pos_encoder(tgt)

          decoder_output = self.decoder(tgt, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask, 
                                        memory_key_padding_mask = memory_key_padding_mask)
          decoder_output = self.linear(decoder_output)
          #print(np.shape(decoder_output))
          #print(decoder_output)

          decoder_output = decoder_output[-1:,:]
          #print(decoder_output)


          token_index = torch.argmax(decoder_output, dim = 2)
          #print(f"shape token index{np.shape(token_index)}")
          #print(f"token index{token_index}")

          #generated_tgt.append(token_index)

          #print(f"shape decoder input{np.shape(decoder_input)}")
          #print(f"shape tocken ind{np.shape(token_index)}")
          token_index = token_index.transpose(1,0)
          #print(f'Token obtained:{token_index}')

          decoder_input = torch.cat([decoder_input, token_index], dim=0)
          tgt = decoder_input


          if np.shape(tgt)[0] == 3:
            break

          #if token_index == id_eos:
           #break

          #if (token_index == id_eos).all().item():
            #break

          #print(tgt)
          #print("tgt")

          #print(np.shape(tgt))
          #print(tgt)
        return tgt


    # grid search implementation for batch mode evaluation

    def greedy_search_batch(self, src, max_len, id_sos, id_eos): #max_len presumabily should be 3, three tokens, sos, place of number, eos

          #forward the encoder once
          src_key_padding_mask = TransformerModel.create_src_padding_mask(src).to(DEVICE)
          memory_key_padding_mask = src_key_padding_mask
          emb = self.embedding_src(src)
          pos_enc = self.pos_encoder(emb)
          memory = self.encoder(pos_enc, src_key_padding_mask=src_key_padding_mask)
          #print(f"memory {memory}")
          
          decoder_input = torch.ones((1,src.shape[1])).fill_(id_sos).type(torch.long).to(DEVICE)
          tgt = decoder_input

          # print(len(memory))

          
          #generated_tgt = [] #we declare a list which is going to contain the generated targets of the model

          while True:

            tgt_key_padding_mask = TransformerModel.create_tgt_padding_mask(tgt).to(DEVICE)
            tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.shape[0]).to(DEVICE)
            tgt = self.embedding_tgt(tgt)
            tgt = self.pos_encoder(tgt)

            decoder_output = self.decoder(tgt, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask, 
                                          memory_key_padding_mask = memory_key_padding_mask)
            decoder_output = self.linear(decoder_output)
            #print(np.shape(decoder_output))
            #print(f"decoder output{torch.argmax(decoder_output, dim = 2)}")

            decoder_output = decoder_output[-1,:,:]
            #print(np.shape(decoder_output))

            #print(decoder_output)


            token_index = torch.argmax(decoder_output, dim = 1)
            #print(f"shape token index{np.shape(token_index)}")
            #print(f"token index{token_index}")

            #generated_tgt.append(token_index)

            #print(f"shape decoder input{np.shape(decoder_input)}")
            #print(f"shape tocken ind{np.shape(token_index)}")
            token_index = token_index[:,None]
            token_index = token_index.transpose(1,0)
            #print(f'Token obtained:{token_index}')

            decoder_input = torch.cat([decoder_input, token_index], dim=0)
            tgt = decoder_input

            # Stopping criteria: the model outputs the end-of-sentence token, eos

            if np.shape(tgt)[0] == 3:
              break

            if token_index.all().item() == id_eos:
             break

          return tgt



    # forward_separate implementation 
    def forward_separate(self, src, tgt):
        """Forward separate function.

        Parameters:
          src: tensor of shape (sequence_length, batch, data dim)
          tgt: tensor of shape (sequence_length, batch, data dim)
        Returns:
          tensor of shape (sequence_length, batch, data dim)
        """
        src_key_padding_mask = TransformerModel(src).to(DEVICE)
        tgt_key_padding_mask = TransformerModel.create_tgt_padding_mask(tgt).to(DEVICE)
        memory_key_padding_mask = src_key_padding_mask
        tgt_mask = nn.Transformer.generate_square_subsequent_mask(
            tgt.shape[0]).to(DEVICE)

        tgt = self.embedding_tgt(tgt)
        tgt = self.pos_encoder(tgt)
        out = self.embedding_src(src)
        out = self.pos_encoder(out)
        out = self.encoder(
            out, src_key_padding_mask=src_key_padding_mask,
            memory_key_padding_mask=memory_key_padding_mask)
        out = self.decoder(
            tgt, out, tgt_mask=tgt_mask,
            tgt_key_padding_mask=tgt_key_padding_mask,
            memory_key_padding_mask=memory_key_padding_mask)
        out = self.linear(out)
        return out

    

    def forward(self, src, tgt):
        """Forward function.

        Parameters:
          src: tensor of shape (sequence_length, batch, data dim)
          tgt: tensor of shape (sequence_length, batch, data dim)
        Returns:
          tensor of shape (sequence_length, batch, data dim)
        """
        src_key_padding_mask = TransformerModel.create_src_padding_mask(src).to(DEVICE)
        tgt_key_padding_mask = TransformerModel.create_tgt_padding_mask(tgt).to(DEVICE)
        memory_key_padding_mask = src_key_padding_mask
        tgt_mask = nn.Transformer.generate_square_subsequent_mask(
            tgt.shape[0]).to(DEVICE)

        tgt = self.embedding_tgt(tgt)
        tgt = self.pos_encoder(tgt)
        out = self.embedding_src(src)
        out = self.pos_encoder(out)
        out = self.transformer(
            out, tgt, src_key_padding_mask=src_key_padding_mask,
            tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask,
            memory_key_padding_mask=memory_key_padding_mask)
        out = self.linear(out)
        return out

# Accuracy computation without batch

def accuracy(model, src, tgt):
    # Initialize a counter for the number of correct predictions
    correct = 0
    
    # Iterate over the input and target sequences
    # print(len(src[1]))
  
    for i in range(src.shape[1]):
        #print(len(src[:,i]))
        # Generate the output sequence using the model's greedy search method
        generated_tgt = model.greedy_search(src[:,i], 3, 3, 2)
        #print(np.shape(generated_tgt))
        generated_tgt = torch.transpose(generated_tgt,1,0)
        #print(np.shape(generated_tgt))
        
        # Compare the generated sequence with the target sequence
        if (generated_tgt == tgt[:,i]).all().item():
            # Increment the counter if the generated sequence is correct
            correct += 1
        #if (i == 0):      # this is just for printing purpose because i wanted to see the difference between the generation and dhe target sequence
          #print(f'TGT {tgt[:,i]}')
          #print(f'generated_tgt {generated_tgt}')

    
    # Compute the accuracy as the number of correct predictions divided by the total number of predictions
    accuracy = 100*(correct / src.shape[1])
    
    return accuracy

# batch accuracy computation.  
# in the training actually we call this function instead of the upper one

def b_accuracy(model, src, tgt):
    # Initialize a counter for the number of correct predictions
    correct = 0
    
    # Iterate over the input and target sequences
    # print(len(src[1]))
  
    
    #print(len(src[:,i]))
    # Generate the output sequence using the model's greedy search method
    generated_tgt = model.greedy_search_batch(src, 3, 3, 2)
    #print(np.shape(tgt))
    #generated_tgt = torch.transpose(generated_tgt,1,0)
    #print(np.shape(generated_tgt))
    
    # Compare the generated sequence with the target sequence
    for i in range(src.shape[1]):
      if (generated_tgt[:,i] == tgt[:,i]).all().item():
          # Increment the counter if the generated sequence is correct
          correct += 1

    #print(f'TGT {tgt[:,1]}')
    #print(f'generated_tgt {generated_tgt[:,1]}')

    
    # Compute the accuracy as the number of correct predictions divided by the total number of predictions
    accuracy = 100*(correct / src.shape[1])
    
    return accuracy

# Experimental hyperparameters

batch_size = 64
clipping = 0.1

train_data_loader = DataLoader(
    dataset=train_set, batch_size=batch_size, shuffle=True)

valid_data_loader = DataLoader(
    dataset=valid_set, batch_size=batch_size, shuffle=False)

learning_rate = 0.0001

model = TransformerModel(source_vocabulary_size = 33, target_vocabulary_size = 14,
                 d_model=256, pad_id=0, encoder_layers=3, decoder_layers=2,
                 dim_feedforward=1024, num_heads=8)

model = model.to(DEVICE)

# training model
import numpy as np

num_epochs = 100

train_loss = []
train_acc = []
val_loss = []
val_acc = []

optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
loss_function = nn.CrossEntropyLoss(ignore_index=0)

for epoch in range(num_epochs):
    print(f"=== start epoch {epoch} ===")
    for batch_idx, (src, tgt) in enumerate(train_data_loader):
      model.train()
      # Define the input tensor
      src = src.permute(1,0)
      #print(np.shape(src))
      tgt = tgt.permute(1,0)
      #print(np.shape(tgt))

      # Zero out the gradients computed in the previous iteration
      output = model(src,tgt[:-1,:])
      out = output.reshape(-1, output.shape[2])
      tgt_output = tgt[1:,:].reshape(-1)
      #print(f"out{out}, tgt_output{tgt_output}")
      loss = loss_function(out, tgt_output)
      loss.backward()
      torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)

      # gradient accumulations
      if(batch_idx % 10 == 0):
        optimizer.step()
        optimizer.zero_grad() 

      if(batch_idx % 100 == 0):
        model.eval()
        acc = b_accuracy(model=model, src=src, tgt=tgt)
        train_loss.append(loss)
        train_acc.append(acc)
        print(f"Batch index:{batch_idx} Epoch:{epoch}, training loss:{loss.item()}, training acc: {acc}%")
        model.train()



        # validation
        model.eval()
        with torch.no_grad():
          intermediate_validation_loss = []
          intermediate_validation_acc = []
          for batch_idx, (src, tgt) in enumerate(valid_data_loader):
            # Define the input tensor
            src = src.permute(1,0)
            #print(np.shape(src))
            tgt = tgt.permute(1,0)
            #print(np.shape(tgt))

            # Zero out the gradients computed in the previous iteration
            output = model(src,tgt[:-1,:])
            out = output.reshape(-1, output.shape[2])
            tgt_output = tgt[1:,:].reshape(-1)
            #print(f"out{out}, tgt_output{tgt_output}")
            loss = loss_function(out, tgt_output)

            v_acc = b_accuracy(model=model, src=src, tgt=tgt)
            intermediate_validation_loss.append(loss)
            intermediate_validation_acc.append(v_acc)
          
          val_loss.append(torch.FloatTensor(intermediate_validation_loss).mean()) 
          val_acc.append(torch.FloatTensor(intermediate_validation_acc).mean())
          #print(intermediate_validation_loss)
          #print(intermediate_validation_acc)
          print(f"Validation loss:{val_loss[-1]}, Validation acc: {val_acc[-1]}%")
        model.train()

# training loss plot 

import matplotlib.pyplot as plt
fig, ax = plt.subplots()

with torch.no_grad():
  plt.plot(torch.tensor(train_loss), '-', color = 'pink')
  ax.set_xlabel("Steps", fontsize=16)
  ax.set_ylabel("Training Loss", fontsize=16)

# training accuracy plot 

import matplotlib.pyplot as plt
fig, ax = plt.subplots()

with torch.no_grad():
  plt.plot(torch.tensor(train_acc), '-', color = 'purple')
  ax.set_xlabel("Steps", fontsize=16)
  ax.set_ylabel("Training Accuracy", fontsize=16)

# validation loss plot 

import matplotlib.pyplot as plt
fig, ax = plt.subplots()

with torch.no_grad():
  plt.plot(torch.tensor(val_loss), '-', color = 'green')
  ax.set_xlabel("Steps", fontsize=16)
  ax.set_ylabel("Validation Loss", fontsize=16)

# validation accuracy plot 

import matplotlib.pyplot as plt
fig, ax = plt.subplots()

with torch.no_grad():
  plt.plot(torch.tensor(val_acc), '-', color = '#89C4F4')
  ax.set_xlabel("Steps", fontsize=16)
  ax.set_ylabel("Validation Loss", fontsize=16)

# Print, check, and report 3 example questions from the validation set and the predictions of your trained model.

model.eval()
for i, (src, tgt) in enumerate(valid_data_loader):
      src = src.transpose(1,0)
      tgt = tgt.transpose(1,0)

      output=model.greedy_search_batch(src,3,3,2)
      output = output.transpose(1,0)
      src = src.transpose(1,0)
 
      if(i<5):
        source_sequence=[]
        target_sequence=[]
        print('--------------------------------')
        for j in src[i]:
            source_sequence.append(src_vocab.id_to_string[j.item()])
        print('src: '+''.join(source_sequence))
        
        for j in output[i]:
            target_sequence.append(trg_vocab.id_to_string[j.item()])
        print('predictions: '+''.join(target_sequence))
